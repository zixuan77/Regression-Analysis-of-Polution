---
title: "PSTAT174Final Project"
author: "Olivia Dong"
date: "2022/3/11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

The industrial production (IP) index measures the real output of all relevant establishments located in the United States, regardless of their ownership, but not those located in U.S. territories. The data consists of industrial production index of electric and gas utilities in the United States, from the years 1985-2018, with our frequency being monthly production output. Our goal for this project is to forecast the future data for the year of 2009-2010 by modeling the time series data of industrial production (IP) index from 1985 to 2018. 

Time series analysis accounts for the fact that data points taken over time may have an trend or seasonal variation that should be accounted for. Therefore, this report includes the process of time series analysis which includes data transformation, model identification, diagnostic checking and forecast. SARIMA model is used to fit this electric production data set. 

the original data is transformed and difference in the first step to get series of a stationary data. Then, acf and pacf plot is used to identify coefficients and fit model.  In the diagnose checking 



```{r, echo=FALSE}
# load package
library(ggplot2)
library(ggfortify)

library(forecast)
library(qpcR)
```
```{r, echo=FALSE}
# read in data
electric.data <- read.csv('Electric_Production.csv')
electric.data[1:293,]
```
```{r, echo=FALSE}

electric <- ts(electric.data[1:293,2],start=c(1985,1),frequency=12)
# visualize time series data
plot.ts(electric)
fit1 <- lm(electric ~ as.numeric(1:length(electric))); abline(fit1, col="red") # add trend line
abline(h=mean(electric), col="blue") # add mean line
```
First, with the time series plot of the original data, we could notice that there is a positive trend and seasonality. Also there is unstable mean and variance. 

```{r, echo=FALSE}
electric.train<-electric[c(1:280)] # training data set
electric.test <-electric[c(281:293)] # testing data set
```
```{r,fig.height=10,fig.width=7, echo=FALSE}
par(mfrow=c(3,1))
plot.ts(electric.train) # plot training data

fit <- lm(electric.train ~ as.numeric(1:length(electric.train))); abline(fit, col="red") 
abline(h=mean(electric.train), col="blue")
hist(electric.train, col="light blue", xlab="", main="histogram: electric.train") # histogram plot

acf(electric.train,lag.max=40, main="ACF of the Electric Data") # acf plot
```

We divide the original data set into two data set: one for training and one for test. The training data set is used for model building. By visualizing the training data with the its time series plot, we could notice a positive trend and seasonality. There exists unstability in variance. There is no apparent skewness in the histogram. However, acfs are large and displays a periodic decay. Therefore, transformation is need to stablize the variance; differencing is need to remove seasonality and trend.
```{r, echo=FALSE}
library(MASS)
bcTransform <- boxcox(electric.train~ as.numeric(1:length(electric.train))) # plot the graph for lambda
lambda=bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
lambda # gives lambda value
electric.bc = (1/lambda)*(electric.train^lambda-1) # use bctransform
```
BcTransform command gives $\lambda=0.303$. $\lambda=0$ also is included in the confidence interval. Try both.

```{r,fig.height=6,fig.width=10, echo=FALSE}
par(mfrow=c(2,2))
plot.ts(electric.bc) 
hist(electric.bc, col="light blue", xlab="", main="histogram: Boxcox transformed electric data")

electric.log =log(electric.train) # gives log transformed data
plot.ts(electric.log)
hist(electric.log, col="light blue", xlab="", main="histogram: log transformed electric data")
```

Comparing the time series plot of the bc transformed data with log transformed data, we could notice that the log transformed data gives a slightly more stable variance. Also, the histograms looks more normal.
```{r, echo=FALSE}
y <- ts(as.ts(electric.log), frequency = 12)
decomp <- decompose(y) # decomposed the log transformed data
plot(decomp)
```
The decomposition of log of training data displays approximately linear trend and seasonality. The period could be 6 or 12. We choose 12 here. Then, we proceed to difference.

```{r,fig.height=10, echo=FALSE}

electric.log12 <- diff(electric.log,lag=12) # difference at lag12
electric.log112 <- diff(electric.log12,lag=1) # difference at lag12 &lag1
par(mfrow=c(3,1))
plot.ts(electric.log, main="log transformed data")
fit <- lm(electric.log ~ as.numeric(1:length(electric.log))); abline(fit, col="red")
abline(h=mean(electric.log), col="blue")

plot.ts(electric.log12, main="log transformed data differenced at lag 12")
fit <- lm(electric.log12 ~ as.numeric(1:length(electric.log12))); abline(fit, col="red")
abline(h=mean(electric.log12), col="blue")

plot.ts(electric.log112, main="log transformed data differenced at lag 12 & lag 1")
fit <- lm(electric.log112 ~ as.numeric(1:length(electric.log112))); abline(fit, col="red")
abline(h=mean(electric.log112), col="blue")

```
We first difference the log transformed data at lag12 to eliminate seasonality. The second plot shows randomness, but there is still a trend. Then, we difference the data at lag1 to eliminate the trend. Finally, the time series plot shows randomness and no trend.

```{r, echo=FALSE}
par(mfrow=c(2,2))
hist(electric.train, col="light blue", xlab="", main="histogram: electric training data")
hist(electric.log, col="light blue", xlab="", main="histogram: log transformed electric data")
hist(electric.log12, col="light blue", xlab="", main="histogram: log transformed electric data,differenced at lag12")
hist(electric.log112, col="light blue", xlab="", main="histogram: log transformed electric data differenced at lag12&lag1")

```
According to the histograms, the data of becomes more concentrated and normal after transformation and differencing.
```{r, echo=FALSE}
var(electric.train) # variance of training data
var(electric.log)  # variance of log transformed training data
var(electric.log12) # variance of training data
var(electric.log112) # variance of training data
```
The variance becomes smaller after every step of transformation and differencing. We are done with the data transformation.

```{r, echo=FALSE}
acf(electric.log,lag.max=40)
acf(electric.log12,lag.max=40)
acf(electric.log112,lag.max=40)
pacf(electric.log112,lag.max=60)
```
The plot of ACF of log transformed training data displays both seasonality and trend. After differencing at lag12, the seasonality of ACF is not apparent now. After difference at both lag12 and lag2, the ACF shows a stationary pattern. There is peak at lag12 and lag24, which indicates that Q might be 1 or 2. ACF seems to be tailing off. Then we have q=2. For plot PACF, There are peaks at seasonal lags at lag=12,24,48. Then we have P=2,4. With PACF tailling off, p might be 2 or 5. We might have MA(24) and AR(48) as well.
P=2,4
p=2,5
Q=1,2
q=2
MA(24)
AR(48)



```{r, echo=FALSE}
# compare the AICc value 
for (p in c(0,5,2))
 {for (j in c(0,2,4))
   { print(p);print(j);print(AICc(arima(electric.log, order=c(p,1,2), seasonal = list(order = c(j,1,2), period = 12),method = "ML")))}}
```

Using a for loop to check model AICc and find the smallest one. We noticed that when we increase the order, the AICc value decrease. 
```{r}
m2 <- arima(electric.log, order=c(5,1,2), seasonal = list(order = c(2,1,1), period = 12),fixed=c(NA,NA,0,0,NA,0,NA,0,NA,NA),method = "ML")
m2
AICc(m2)
```
```{r}
polyroot(c(1,-0.3846, 0.4247,0,0 ,-0.1191))
```

The model of $SARIMA(5,1,2)(2,1,1)$ has the lowest AICc. After fixing the zero coefficients, the AICc value becomes -1244.265. However, the model is not stationary by cheching the polynomial roots of ar part. 

```{r, echo=FALSE}

m0 <- arima(electric.log, order=c(2,1,2), seasonal = list(order = c(2,1,2), period = 12),method = "ML")
m0
AICc(m0)
AICc(arima(electric.log, order=c(2,1,2), seasonal = list(order = c(2,1,1), period = 12),method = "ML"))
```
```{r, echo=FALSE}
# choose the model with lower aicc
m1 <- arima(electric.log, order=c(2,1,2), seasonal = list(order = c(2,1,1), period = 12),fixed=c(0,NA,NA,NA,0,NA,NA),method = "ML")
m1
AICc(m1)
```


```{r, echo=FALSE}
polyroot(c(1,-0.3960,-0.5058)) # check invertibility
```
Model $SARIMA(2,1,2)(2,1,2)$ with the second lowest AICc is then tried. The coefficient on of SMA2 has zero within its confidence interval. Therefore, I reduce the value of Q by 1. Then I have $SARIMA(2,1,2)(2,1,1)$ as my time series model. The AICc value decreases. There is still zero's included in the confidence interval of coefficients at sar1, ma1 and ar1. The AICc is now reduced to 1237.987. Then I fixed these three coefficients to zero one at a time. After I fixed the sar1 as 0. The AICc becomes -1239.762. Then I also fix the ar1 to be 0. The AICC becomes -1241.635. Now, all other coefficients are significant. The final model chosen is $SARIMA(2,1,2)(2,1,2)$. It is invertible and stationary. 
```{r, echo=FALSE}

res <- residuals(m1) # return residuals of m1
hist(res,density=20,breaks=20, col="blue", xlab="", prob=TRUE) # histogram of residuals
m <- mean(res)
std <- sqrt(var(res))
curve(dnorm(x,m,std), add=TRUE)
```
The histogram of residuals is approximately normal except that there is outlier on the left which forms a heavy tail. This is also displayed in the Q-Q plot: most points follows the straight line, but some points on the left part deviates from the line. 

```{r, echo=FALSE,fig.height=8,fig.width=10}
par(mfrow=c(1,2))
plot.ts(res)
fitt <- lm(res ~ as.numeric(1:length(res))); abline(fitt, col="red")
abline(h=mean(res), col="blue")
qqnorm(res,main= "Normal Q-Q Plot for Model M1")
qqline(res,col="blue")
acf(res, lag.max=40,main= "ACF for Model M1")
pacf(res, lag.max=40,main= "PACF for Model M1")
```
The time series plot of residuals shows a randomness with no apparent trend and seasonality. The mean(blue) and regression line(red) looks close and approximately overlaps y=0. The ACFs and PACFs of residuals are within confidence intervals so that they can be counted as zeros.
```{r, echo=FALSE}
shapiro.test(res)
Box.test(res, lag = 12, type = c("Box-Pierce"), fitdf = 7)
Box.test(res, lag = 12, type = c("Ljung-Box"), fitdf = 7)
Box.test(res^2, lag = 12, type = c("Ljung-Box"), fitdf = 0)
acf(res^2, lag.max=40)
ar(res, aic = TRUE, order.max = NULL, method = c("yule-walker"))
```
Model M1 passes all the tests except Shapiro-Wilk normality test. This might because the heavy tail we have noticed in histogram and Q-Q plot. Then, we fit the model to white noise.

Finally, use the model $SARIMA(2,1,2)(2,1,1)_12$ to forcast the next 12 values. 
Final Model for the logarithm transform of original data:
ln(Y_t) follows $SARIMA (2,1,2)(2,1,1)_12$ model:
$(1+0.2440B^2)(1-0.1771B^{24}) \nabla_1 \nabla_{12}ln(Y_t)=(1-0.3960B -0.5058B^2)(1-0.7301B^{12})Z_t$
```{r, echo=FALSE}


forecast(m1)
pred.tr <- predict(m1, n.ahead = 12)
U.tr= pred.tr$pred + 2*pred.tr$se
L.tr= pred.tr$pred - 2*pred.tr$se 
ts.plot(electric.log, xlim=c(1,length(electric.log)+12), ylim = c(min(electric.log),max(U.tr)))
lines(U.tr, col="blue", lty="dashed")
lines(L.tr, col="blue", lty="dashed")
points((length(electric.log)+1):(length(electric.log)+12), pred.tr$pred, col="red")
```


```{r, echo=FALSE}
pred.orig <- exp(pred.tr$pred)
U= exp(U.tr)
L= exp(L.tr)
ts.plot(electric.train, xlim=c(1,length(electric.train)+12), ylim = c(min(electric.train),max(U)))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points((length(electric.train)+1):(length(electric.train)+12), pred.orig, col="red")

```
```{r, echo=FALSE}
ts.plot(electric.train, xlim = c(200,length(electric.train)+12), ylim = c(70,max(U)))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points((length(electric.train)+1):(length(electric.train)+12), pred.orig, col="red")
```

```{r, echo=FALSE} 
len_data <- length(c(electric.train,electric.test))

ts.plot(c(electric.train,electric.test), xlim = c(200,length(electric.train)+12), ylim = c(70,max(U)), col="red")
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points((length(electric.train)+1):(length(electric.train)+12), pred.orig, col="green")
points((length(electric.train)+1):(length(electric.train)+12), pred.orig, col="black")
```

The test data corresponding to red line is within the confidence interval with some parts close to the lower boundary. Also, we could notice that the confidence internal is relatively narrow. This two features of forecast reflect the influence of heavy tail on the lower part of histogram of residuals and deviation in Q-Q plot. This might also consistent with the small p-value for the Shapiro-Wilk test. The residual is not symetric and normal.

## Conclusion
ln(Y_t) follows $SARIMA (2,1,2)(2,1,1)_12$ model:
$(1+0.2440B^2)(1-0.1771B^{24}) \nabla_1 \nabla_{12}ln(Y_t)=(1-0.3960B -0.5058B^2)(1-0.7301B^{12})Z_t$
## Reference

\newpage
## Code appendix

```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```